# SSE 实现进度记录

## 2025-01-04 任务创建

### 完成的工作
1. **需求调研**
   - 分析了 PRD 文档中的 SSE 需求
   - 查看了 REST API 规范中的 SSE 端点定义
   - 研究了异步任务数据库设计中的 SSE 集成方案

2. **技术栈分析**
   - 确认 FastAPI 版本支持 SSE（v0.115.13）
   - 检查了前端依赖，确认需要实现 EventSource 客户端
   - 验证了 Kafka 和 Redis 可用于事件分发

3. **架构设计**
   - 设计了分层的 SSE 架构
   - 绘制了系统架构图、流程图、时序图和类图
   - 制定了详细的实现计划

### 关键决策
- 选择使用 FastAPI 的 StreamingResponse 实现 SSE
- 采用 EventBus 模式统一事件分发
- 前端使用 React Context 管理 SSE 连接
- 实现自动重连机制提高可靠性

### 架构决策记录

**SSE 事件模型位置** - 决定将 SSE 事件模型放在 `packages/shared-types/src/sse_events.py`：
- 与 Kafka 事件（`events.py`）分离，避免概念混淆
- SSE 事件遵循 W3C 标准格式，与 Kafka 事件格式不同
- 便于前后端共享类型定义，保持类型安全

### 下一步计划
1. 创建 `packages/shared-types/src/sse_events.py` 文件
2. 定义 SSE 事件基类和具体事件类型
3. 更新 TypeScript 类型定义
4. 创建后端 SSE 基础架构
5. 实现 SSEManager 核心类

---

## 2025-01-04 架构评审和改进

### 评审反馈总结
收到了专业的架构评审，涵盖五个层面：
1. 宏观架构与系统契合度
2. 关键技术细节
3. 交互契约与事件模型
4. 运维、安全、可观测性
5. 任务拆解与进度管理

### 主要改进项（已完成）

**P0 - 立即实施**：
1. ✅ 更新 SSE 事件模型，添加 `scope`、`version` 字段
2. ✅ 创建 Kafka → SSE 映射文档（`docs/development/sse-event-mapping.md`）
3. ✅ 设计事件补偿机制（Last-Event-ID + offset replay）
4. ✅ 在架构中新增 Event Adapter 层

**P1 - 实现前完善**：
1. ✅ 定义 Prometheus 监控指标
2. ✅ 设计 Grafana Dashboard 需求
3. ✅ 更新任务清单，按优先级重新组织

**P2 - 上线前优化**：
1. ✅ 添加负载均衡器配置示例
2. ✅ 制定灰度发布策略
3. ✅ 定义 E2E 测试场景

### 架构关键决策
1. **分层清晰**：API 层 → SSEManager → Adapter → EventBus → 事件源
2. **事件补偿**：使用 `{source}:{partition}:{offset}` 格式的事件 ID
3. **性能优化**：背压机制、心跳保活、连接限制
4. **可观测性**：完整的指标、日志、追踪方案

### 风险和缓解措施
1. **高并发风险**：通过连接池限制和背压机制缓解
2. **网络不稳定**：实现智能重连和事件缓存
3. **安全风险**：JWT 认证 + 速率限制 + 权限检查

### 下一步行动
1. 开始 P0 任务实施（预计 29.5 小时）
2. 首先创建 SSE 事件模型和 TypeScript 类型
3. 实现 EventBus 接口和 Adapter 层
4. 逐步构建 SSEManager 和端点

---

## 2025-01-04 MVP 文档调整

### 调整背景
根据项目当前处于 MVP 阶段的实际情况，调整 SSE 实现方案文档，去除暂时不需要的高级功能。

### 第一次调整：移除高级功能
1. **Prometheus 指标 & Grafana Dashboard**
   - 监控指标收集
   - Grafana 可视化面板
   - 这些功能在生产环境部署时再考虑

2. **Nginx/负载均衡器配置**
   - 复杂的反向代理配置
   - 长连接参数优化
   - MVP 阶段使用默认配置即可

3. **高级性能优化**
   - 背压机制（队列溢出控制）
   - 心跳保活（每15秒发送保活信号）
   - 连接上限控制（单用户最大连接数）
   - 这些在用户量增长后再实施

4. **灰度发布策略**
   - 版本标识机制
   - 连接迁移方案
   - MVP 阶段不需要复杂的发布流程

### 第二次调整：重新思考 P0/P1 划分

#### 重新定义的 P0 (真正的 MVP - 16小时)
1. **基础事件模型** - 仅包含核心事件类型
2. **后端最小实现** - 简单的 SSEManager 和端点
3. **前端最小实现** - 基础的 EventSource 客户端
4. **基础测试** - 手动测试流程和一个端到端测试

#### 重新定义的 P1 (功能完善 - 31小时)
1. **完整的事件系统** - EventBus 抽象、Kafka 集成
2. **高级连接管理** - 队列管理、简单的事件补偿
3. **前端增强** - 完善的重连策略、状态管理
4. **测试覆盖** - 单元测试和集成测试
5. **日志与文档** - 结构化日志和开发文档

#### P2 保持为生产优化 (24小时)
- 性能优化、高级功能、安全加固

### 关键决策
- **前端实现移到 P0**：没有前端，SSE 就无法验证
- **简化 P0 实现**：去除所有抽象层，直接实现
- **推迟复杂功能**：事件补偿、EventBus 等移到 P1

### 第三次调整：恢复必要的技术特性

根据技术评审反馈，以下功能对 SSE 正常工作是必要的，需要调整优先级：

#### 调整到 P0 的功能
- **心跳机制**：没有心跳，SSE 连接很容易断开
- **基础的 TanStack Query 集成**：前端需要自动更新缓存

#### 保留在 P1 的功能
- **背压处理**：队列满时的处理策略
- **事件补偿机制**：Last-Event-ID 和事件重放
- **多标签订阅**：动态 filter set
- **连接数限制**：--max-clients 参数
- **完整的监控指标**：虽然不用 Prometheus，但基础指标仍需要

#### 技术建议整合
1. 使用 uvloop 或 trio 降低调度开销
2. 内存估算：10k 连接约需 200MB
3. 前端 30 秒无数据自动重连
4. 约定 entity_type + action 字段格式

### 第四次调整：完善事件定义细节

根据进一步的技术反馈，补充了事件定义的重要细节：

#### 事件命名规范
- 使用 `kebab-case.verb-past` 格式（如 `task.progress-updated`）
- 与 Kafka DomainEvent 命名保持一致
- 错误事件使用 `error.` 前缀

#### Scope 字段用于前端路由
- `user` - 用户级别事件
- `session` - 会话级别事件  
- `novel` - 小说级别事件
- `global` - 全局广播事件

#### Kafka → SSE 映射表
- 添加了详细的映射表，包含过滤 key 和 scope
- 明确了数据转换规则
- 特殊处理错误事件

#### 错误事件定义
- 新增 `ErrorEvent` 类型
- 包含 level、code、message、correlation_id 字段
- 统一前端错误处理

### 下一步计划
按照完善后的事件定义开始实施，确保前后端事件契约清晰一致。

---

## 待更新...

（后续实施进度将在此记录）